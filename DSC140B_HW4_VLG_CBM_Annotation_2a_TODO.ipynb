{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NGLYRY/Final-Proj-Dsc106/blob/main/DSC140B_HW4_VLG_CBM_Annotation_2a_TODO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycb7rrFeT-Fp"
      },
      "source": [
        "# Concept Bottleneck Models and Automated Annotation using Grounding DINO\n",
        "\n",
        "In this notebook, you will learn how to automatically generate concept annotations using a foundation model for open-vocabulary object detection called [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO). This pipeline mimics the approach proposed in VLG-CBM for reducing manual concept labeling.\n",
        "\n",
        "We will:\n",
        "- Load and configure a pre-trained Grounding DINO model\n",
        "- Define interpretable concept sets for a target class\n",
        "- Generate bounding box annotations for an input image\n",
        "- Visualize annotations across multiple confidence thresholds\n",
        "\n",
        "Let's begin!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbIcMPYAbikT"
      },
      "outputs": [],
      "source": [
        "# Install required libraries and Grounding DINO\n",
        "# IMPORTANT: DO NOT MODIFY except the lines marked TODO\n",
        "\n",
        "HOME_DIR = ....  # TODO: Set HOME_DIR location\n",
        "!pip install -q torch torchvision matplotlib\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git {HOME_DIR}\n",
        "%cd {HOME_DIR}/groundingdino/models/GroundingDINO/csrc/MsDeformAttn\n",
        "!sed -i 's/value.type()/value.scalar_type()/g' ms_deform_attn_cuda.cu\n",
        "!sed -i 's/value.scalar_type().is_cuda()/value.is_cuda()/g' ms_deform_attn_cuda.cu\n",
        "%cd {HOME_DIR}\n",
        "!pip install -q -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNGIgGa0EGLU"
      },
      "outputs": [],
      "source": [
        "# TODO: Move files in DSC140B_HW4_VLG_CBM_Annotation_2a.zip to HOME_DIR\n",
        "import os\n",
        "os.chdir(HOME_DIR)\n",
        "print(\"Current working directory: \", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTJL1lOsbPiD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "from groundingdino.util.slconfig import SLConfig\n",
        "from groundingdino.util.utils import clean_state_dict\n",
        "from groundingdino.models import build_model\n",
        "import groundingdino.datasets.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdggRLzR_GKP"
      },
      "outputs": [],
      "source": [
        "# Check device in use\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c28qUNXtZ0GV"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: DO NOT MODIFY THIS CELL\n",
        "seed =42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6LiYhNKUYYz"
      },
      "source": [
        "### Load the Pre-trained Grounding DINO Model\n",
        "\n",
        "We now load the model checkpoint and configuration. This will allow us to perform open-vocabulary object detection based on text prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCkNw4uJUbva"
      },
      "outputs": [],
      "source": [
        "def load_annotation_model(config_path, checkpoint_path, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Loads the Grounding DINO model and tokenizer.\n",
        "\n",
        "    Args:\n",
        "        config_path (str): Path to the model configuration file.\n",
        "        checkpoint_path (str): Path to the model weights (.pth file).\n",
        "        device (str): Device to load model on (\"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "        model (torch.nn.Module): Grounding DINO model.\n",
        "        tokenizer: Tokenizer used for prompt encoding.\n",
        "    \"\"\"\n",
        "    args = SLConfig.fromfile(config_path)\n",
        "    args.device = device\n",
        "    model = build_model(args)\n",
        "    checkpoint = .... # TODO: load from checkpoint and map to CPU\n",
        "    model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
        "    .... # TODO: Set model into eval mode and move to device\n",
        "    return model, model.tokenizer\n",
        "\n",
        "# Download checkpoint if not already present\n",
        "if not os.path.exists(\"groundingdino_swinb_cogcoor.pth\"):\n",
        "    !wget https://huggingface.co/ShilongLiu/GroundingDINO/resolve/main/groundingdino_swinb_cogcoor.pth \\\n",
        "        -O groundingdino_swinb_cogcoor.pth\n",
        "\n",
        "# Load model\n",
        "model_config_path = \"groundingdino/config/GroundingDINO_SwinB_cfg.py\"\n",
        "model_checkpoint_path = \"groundingdino_swinb_cogcoor.pth\"\n",
        "device = .... # TODO: Set device based on the torch.cuda.is_available() function\n",
        "\n",
        "model, tokenizer = load_annotation_model(model_config_path, model_checkpoint_path, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnOBELsHWYpB"
      },
      "source": [
        "### Setup Grounding DINO prompt\n",
        "\n",
        "A **concept set** for a class in a Concept Bottleneck Model (CBM) refers to a collection of interpretable and human-understandable attributes that characterize the class. The image provided belongs to the class *Black-footed Albatross*, and its associated concept set is defined below.\n",
        "\n",
        "To ensure that each concept (e.g., “black feet”, “long wings”) is treated as a distinct entity by Grounding DINO, we concatenate concepts using periods (`\".\"`) as delimiters. This punctuation helps the model tokenize and attend to each phrase individually during detection.\n",
        "\n",
        "Additionally, including the class name (e.g., *Black-footed Albatross*) at the beginning of the prompt forces the model to focus on attributes specific to that class with greater precision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G25isssRbsVz"
      },
      "outputs": [],
      "source": [
        "CLASS_LABEL = \"Black footed Albatross\"\n",
        "\n",
        "# Define concepts associated with the Black footed Albatross class\n",
        "CONCEPT_SET = [\n",
        "    \"black feet\",\n",
        "    \"dark wingtips\",\n",
        "    \"large size\",\n",
        "    \"large wingspan\",\n",
        "    \"long wings\",\n",
        "    \"white body\",\n",
        "    \"yellow beak\",\n",
        "    \"yellow bill\"\n",
        "]\n",
        "\n",
        "# Construct prompt: Class name followed by dot-separated concepts\n",
        "prompt = CLASS_LABEL + \".\" + \" . \".join(CONCEPT_SET)\n",
        "print(\"Grounding DINO Prompt:\", prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7j_Ntj4Ym7Y"
      },
      "source": [
        "### Load Image and Apply Transforms\n",
        "\n",
        "We prepare the input image using standard transformations expected by Grounding DINO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WscYwDRPbyoi"
      },
      "outputs": [],
      "source": [
        "# TODO: Define transform that resize images to 800x800, converts the image to tensor, and then\n",
        "# applies the following normalization: MEAN: [0.485, 0.456, 0.406] and STD: [0.229, 0.224, 0.225]\n",
        "transform = transforms.Compose([\n",
        "    ...,\n",
        "    ...,\n",
        "    ...\n",
        "])\n",
        "\n",
        "raw_transform = transforms.Compose([transforms.Resize((800, 800))])\n",
        "image_pil = Image.open(f\"/{HOME_DIR}/Black_Footed_Albatross.jpg\")  # TODO: LOAD PIL IMAGE\n",
        "image_pil = raw_transform(image_pil)\n",
        "image_tensor = ... # TODO: Apply transform to image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYqLUE3grOi4"
      },
      "source": [
        "## Run Inference with Grounding DINO and process annotations\n",
        "\n",
        "Now that we have a preprocessed image and a natural language prompt, we can run inference using the Grounding DINO model and process the model output. We define a threshold (`THRESHOLD`) that determines whether a predicted concept-region pair is confident enough to be included in the final annotations. The threshold is applied to a \"perplexity-like\" confidence score computed from the logits for each concept span."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwXc_uU9b385"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, image_tensor, prompts):\n",
        "    \"\"\"\n",
        "    Runs inference on the image and returns prediction logits and boxes.\n",
        "\n",
        "    Args:\n",
        "        model: Grounding DINO model\n",
        "        image_tensor: Normalized tensor of shape (1, 3, H, W)\n",
        "        prompts: List of prompt strings\n",
        "\n",
        "    Returns:\n",
        "        logits (Tensor): Raw concept confidence scores\n",
        "        boxes (Tensor): Predicted bounding boxes\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor, captions=prompts)\n",
        "    return outputs[\"pred_logits\"].sigmoid(), outputs[\"pred_boxes\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqpGAyepb6Ox"
      },
      "outputs": [],
      "source": [
        "def process_annotations(image_pil, prompt, logits, boxes, tokenizer, threshold=0.35):\n",
        "    \"\"\"\n",
        "    Processes model outputs to extract bounding boxes and concept labels based on a confidence threshold.\n",
        "\n",
        "    Args:\n",
        "        image_pil (PIL.Image): Original (resized) image for which annotations are generated.\n",
        "        prompt (str): Prompt string containing the class and dot-separated concepts.\n",
        "        logits (torch.Tensor): Output logits from Grounding DINO (after sigmoid), shape: (1, num_boxes, num_tokens).\n",
        "        boxes (torch.Tensor): Normalized bounding boxes from Grounding DINO, shape: (1, num_boxes, 4).\n",
        "        tokenizer: Tokenizer used to tokenize the prompt.\n",
        "        threshold (float): Per-concept threshold for including bounding boxes based on perplexity (confidence proxy).\n",
        "\n",
        "    Returns:\n",
        "        annotations (list of dict): List of dictionaries with keys:\n",
        "            - \"concept\": The human-readable concept string\n",
        "            - \"box\": Bounding box in [x_min, y_min, x_max, y_max] format, scaled to image size\n",
        "    \"\"\"\n",
        "    annotations = []\n",
        "    W, H = image_pil.size  # Note: PIL uses (width, height)\n",
        "\n",
        "    # Convert model outputs to NumPy arrays\n",
        "    logits = logits[0].cpu().numpy()\n",
        "    boxes = boxes[0].cpu().numpy()\n",
        "\n",
        "    # Tokenize prompt and remove start/end tokens\n",
        "    prompt_tokenized = tokenizer(prompt)[\"input_ids\"][1:-1]\n",
        "    logits = logits[:, 1:-1]  # Remove logits for special tokens\n",
        "\n",
        "    # Identify concept boundaries in token sequence (period token has ID 1012)\n",
        "    split_indices = [i for i, token_id in enumerate(prompt_tokenized) if token_id == 1012]\n",
        "    start = 0\n",
        "\n",
        "    for split in split_indices:\n",
        "        # Slice out tokens for one concept\n",
        "        concept_ids = prompt_tokenized[start:split]\n",
        "        concept_text = tokenizer.decode(concept_ids).strip()\n",
        "\n",
        "        # Get the logits associated with this concept span\n",
        "        concept_logits = logits[:, start:split]\n",
        "\n",
        "        for j in range(len(concept_logits)):\n",
        "            # Approximate confidence using geometric mean (perplexity-like)\n",
        "            prob = np.prod(concept_logits[j])\n",
        "            perplexity = prob ** (1 / len(concept_ids)) if len(concept_ids) > 0 else 0\n",
        "\n",
        "            if perplexity > threshold:\n",
        "                # Convert box from [cx, cy, w, h] to [x0, y0, x1, y1] in image coordinates\n",
        "                box = boxes[j]\n",
        "                box[[0, 2]] *= W\n",
        "                box[[1, 3]] *= H\n",
        "                box[0] -= box[2] / 2  # x0 = cx - w/2\n",
        "                box[1] -= box[3] / 2  # y0 = cy - h/2\n",
        "                box[2] += box[0]      # x1 = x0 + w\n",
        "                box[3] += box[1]      # y1 = y0 + h\n",
        "\n",
        "                # Save annotation\n",
        "                annotations.append({\n",
        "                    \"concept\": concept_text,\n",
        "                    \"box\": box\n",
        "                })\n",
        "\n",
        "        start = split + 1  # Move to next concept span\n",
        "\n",
        "    return annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRyCHtCXsObe"
      },
      "source": [
        "## Visualize Annotations\n",
        "\n",
        "Now that we have processed annotations we can visualize the annotated image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R6YX5T4sHey"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZoJnkYab7YJ"
      },
      "outputs": [],
      "source": [
        "def plot_annotations(image, annotations, title=None):\n",
        "    \"\"\"\n",
        "    Annotate a PIL image with bounding boxes and concept labels using PIL's drawing utilities,\n",
        "    and display the result using IPython display.\n",
        "\n",
        "    Args:\n",
        "        image (PIL.Image): Input image to annotate.\n",
        "        annotations (list of dict): Each dict must contain 'box' (xyxy format) and 'concept'.\n",
        "        title (str, optional): Optional title to print before displaying the image.\n",
        "    \"\"\"\n",
        "    image_copy = image.copy()\n",
        "    draw = ImageDraw.Draw(image_copy)\n",
        "\n",
        "    # Optional: Load a font (fallback to default if unavailable)\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", size=12)\n",
        "    except IOError:\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    for ann in annotations:\n",
        "        box = ann[\"box\"]\n",
        "        concept = ann[\"concept\"]\n",
        "        try:\n",
        "          draw.rectangle(box, outline=\"red\", width=2)\n",
        "          draw.text((box[0], box[1]), concept, fill=\"black\", font=font)\n",
        "        except ValueError:\n",
        "          pass\n",
        "\n",
        "    if title:\n",
        "        print(title)\n",
        "    display(image_copy)\n",
        "\n",
        "# plot image with annotations\n",
        "THRESHOLDS = [0.1, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6]\n",
        "logits, boxes = .... # TODO: Get predictions. Note that the models take a batch as input\n",
        "for THRESHOLD in THRESHOLDS:\n",
        "  annotations = process_annotations(image_pil, prompt, logits.cpu().detach().clone(), boxes.cpu().detach().clone(), tokenizer, threshold=THRESHOLD)\n",
        "  print(\"Plotting for thresold: \", THRESHOLD)\n",
        "  plot_annotations(image_pil, annotations)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
